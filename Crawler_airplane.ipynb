{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import time \n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#傻眼Ing,SQL吃不了表情文字，需要額外刪除表情文字。python竟然可以讀部分XD，厲害\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立log，紀錄資訊\n",
    "date_time=datetime.strftime(datetime.now(),'%Y-%m-%d')\n",
    "path='C:/Users/lenovon/Desktop/Test/'+date_time+'.log'\n",
    "if os.path.isfile(path):\n",
    "    logging.basicConfig(level=logging.WARNING,#控制檯列印的日誌級別\n",
    "                        filename=path,\n",
    "                        filemode='a') #模式有w和a，w就是寫模式，每次都會重新寫日誌，覆蓋之前的日誌;a是append的意思\n",
    "else:\n",
    "    logging.basicConfig(level=logging.WARNING,#控制檯列印的日誌級別\n",
    "                        filename=path,\n",
    "                        filemode='w') #模式有w和a，w就是寫模式，每次都會重新寫日誌，覆蓋之前的日誌;a是append的意思"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#header\n",
    "user_agent = [\n",
    "\"Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\",\n",
    "\"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\",\n",
    "\"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0\",\n",
    "\"Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; .NET4.0C; .NET4.0E; .NET CLR 2.0.50727; .NET CLR 3.0.30729; .NET CLR 3.5.30729; InfoPath.3; rv:11.0) like Gecko\",\n",
    "\"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)\",\n",
    "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\",\n",
    "\"Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\",\n",
    "\"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11\",\n",
    "\"Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11\",\n",
    "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; TencentTraveler 4.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; The World)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Avant Browser)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get article url for 1 to page_num\n",
    "def get_article_url(url,mode,to_page=None):\n",
    "    Links=[]\n",
    "    if mode ==1: #Simple_flying       \n",
    "        #crawl 1~to_page url of articles\n",
    "        for j in range(1,to_page+1):\n",
    "            a=requests.post('https://simpleflying.com/wp-json/csco/v1/more-posts',\n",
    "                    data={\n",
    "                        'page':j,'action':'csco_ajax_load_more',\n",
    "                        'posts_per_page':12,\n",
    "                        'query_data': '{\"location\":\"home\",\"infinite_load\":false,\"query_vars\":{\"error\":\"\",\"m\":\"\",\"p\":0,\"post_parent\":\"\",\"subpost\":\"\",\"subpost_id\":\"\",\"attachment\":\"\",\"attachment_id\":0,\"name\":\"\",\"pagename\":\"\",\"page_id\":0,\"second\":\"\",\"minute\":\"\",\"hour\":\"\",\"day\":0,\"monthnum\":0,\"year\":0,\"w\":0,\"category_name\":\"\",\"tag\":\"\",\"cat\":\"\",\"tag_id\":\"\",\"author\":\"\",\"author_name\":\"\",\"feed\":\"\",\"tb\":\"\",\"paged\":0,\"meta_key\":\"\",\"meta_value\":\"\",\"preview\":\"\",\"s\":\"\",\"sentence\":\"\",\"title\":\"\",\"fields\":\"\",\"menu_order\":\"\",\"embed\":\"\",\"category__in\":[],\"category__not_in\":[],\"category__and\":[],\"post__in\":[],\"post__not_in\":[59602,59548,59586,50517],\"post_name__in\":[],\"tag__in\":[],\"tag__not_in\":[],\"tag__and\":[],\"tag_slug__in\":[],\"tag_slug__and\":[],\"post_parent__in\":[],\"post_parent__not_in\":[],\"author__in\":[],\"author__not_in\":[],\"49f47c948536c05fb3145f9e3a863558\":[59602],\"64c05e4b4d134438a75e15c6d2e05464\":[59548,59586,50517],\"ignore_sticky_posts\":false,\"suppress_filters\":false,\"cache_results\":true,\"update_post_term_cache\":true,\"lazy_load_term_meta\":true,\"update_post_meta_cache\":true,\"post_type\":\"\",\"posts_per_page\":12,\"nopaging\":false,\"comments_per_page\":\"50\",\"no_found_rows\":false,\"order\":\"DESC\"},\"in_the_loop\":false,\"is_single\":false,\"is_page\":false,\"is_archive\":false,\"is_author\":false,\"is_category\":false,\"is_tag\":false,\"is_tax\":false,\"is_home\":true,\"is_singular\":false}'\n",
    "                    },\n",
    "                   headers={\n",
    "                       'origin':'https://simpleflying.com',\n",
    "                       'referer':'https://simpleflying.com/category/aviation-news/',\n",
    "                       'content-type':'application/x-www-form-urlencoded;charset=UTF-8',\n",
    "                       'User-Agent':np.random.choice(user_agent)})\n",
    "            if a.status_code !=200:\n",
    "                s=datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S')\n",
    "                logging.error(s+':'+' post method for page whose status_code is not 200, please check.'+str(j)+' page')\n",
    "            temp=a.json()\n",
    "            soup=BeautifulSoup(temp['data']['content'],'lxml')\n",
    "            for s in soup.find_all(class_='post-inner'):\n",
    "                try:\n",
    "                    temp=s.find('a').get('href')\n",
    "                    Links.append(temp)\n",
    "                except:\n",
    "                    continue\n",
    "            #delete duplicated url\n",
    "            Links=[s for s in np.unique(Links)]\n",
    "            time.sleep(4)\n",
    "    elif mode ==2: #Reuters\n",
    "        a=requests.get(url,timeout=20,\n",
    "                   headers={'User-Agent':np.random.choice(user_agent)})\n",
    "        if a.status_code !=200:\n",
    "            s=datetime.strftime(datetime.now(),'%Y-%m-%d %H:%M:%S')\n",
    "            logging.error(s+':'+' status_code is not 200, url is : '+url)\n",
    "        soup=BeautifulSoup(a.text,'lxml')\n",
    "        try:\n",
    "            Links=['https://www.reuters.com/'+s.find('a').get('href') for s in soup.find('div',class_='column1 col col-10').find_next('div',class_='news-headline-list').find_all(class_='story-content')]\n",
    "        except:\n",
    "            print('Can not find any article url for this url,please check.')\n",
    "            Links=[]\n",
    "    else: # FlightGlobal\n",
    "        a=requests.get(url,timeout=20,\n",
    "                   headers={'User-Agent':np.random.choice(user_agent)})\n",
    "        if a.status_code !=200:\n",
    "            s=datetime.strftime(datetime.now(),'%Y-%m-%d %H:%M:%S')\n",
    "            logging.error(s+':'+' status_code is not 200, url is : '+url)\n",
    "        soup=BeautifulSoup(a.text,'lxml')\n",
    "        try:\n",
    "            Links=[s.find('a').get('href') for s in soup.find_all('h3')]\n",
    "        except:\n",
    "            print('Can not find any article url for thiss url,please check.')\n",
    "            Links=[]\n",
    "    return(Links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2=['https://www.reuters.com/news/archive/businessnews?view=page&page=1&pageSize=10',\n",
    "      'https://www.reuters.com/news/archive/marketsnews?view=page&page=1&pageSize=10',\n",
    "      'https://www.reuters.com/news/archive/worldnews?view=page&page=1&pageSize=10',\n",
    "      'https://www.reuters.com/news/archive/politicsnews?view=page&page=1&pageSize=10',\n",
    "      'https://www.reuters.com/news/archive/mcbreakingviews?view=page&page=1&pageSize=10',\n",
    "      'https://www.reuters.com/news/archive/personalfinance?view=page&page=1&pageSize=10']\n",
    "url3=['https://www.flightglobal.com/236.more?page=1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Links1=get_article_url(to_page=3,url=None,mode=1)\n",
    "    Links2=[]\n",
    "    for u in url2:\n",
    "        uu=get_article_url(url=u,mode=2)\n",
    "        for s in uu:\n",
    "            Links2.append(s)\n",
    "        time.sleep(2)\n",
    "    Links3=[]\n",
    "    for u in url3:\n",
    "        uu=get_article_url(url=u,mode=3)\n",
    "        for s in uu:\n",
    "            Links3.append(s)\n",
    "except:\n",
    "    logging.error('Function:get_article_url is wrong,please check.',exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crawl any useful information of article\n",
    "def crawl_info(article_url,index,mode):\n",
    "    if mode ==1: #Simple_Flying\n",
    "        a=requests.get(article_url,timeout=20)\n",
    "        if a.status_code !=200:\n",
    "            s=datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S')\n",
    "            logging.error(s+':'+' some status_code of url is not 200, please check.\\n'+article_url)\n",
    "        soup=BeautifulSoup(a.text,'lxml')\n",
    "        try:\n",
    "            title=soup.find(class_='entry-header-inner').find('h1').text\n",
    "        except:\n",
    "            title=' '\n",
    "        try:\n",
    "            author=soup.find(class_='entry-header-inner').find(class_='author').text\n",
    "        except:\n",
    "            author=' '\n",
    "        #SQL 的麻煩資料時間格式\n",
    "        try:\n",
    "            dt_publish=soup.find(class_='entry-header-inner').find(class_='meta-date').text\n",
    "            dt_publish=datetime.strftime(datetime.strptime(dt_publish,'%B %d, %Y'),'%Y/%m/%d')\n",
    "        except:\n",
    "            dt_publish='NULL'\n",
    "        try:\n",
    "            temp=soup.find(class_='wprt-container').find_all('p')\n",
    "            temp_c=[s.text for s in temp]\n",
    "            #ignore 1 and find first position of subtitle in content\n",
    "            sub_header=[s.find_previous('h3') for s in temp]\n",
    "            sub_header_logical=pd.DataFrame(sub_header,columns=['V1']).duplicated().tolist()\n",
    "            if len(temp_c) != len(sub_header):\n",
    "                raise RuntimeError('Error for length of content not equal to length of boolean of sub_header')\n",
    "            else:\n",
    "                content=''\n",
    "                for i in range(0,len(temp_c)):\n",
    "                    if not sub_header_logical[i] and i !=0:\n",
    "                        content=content+'\\n'+sub_header[i].text+'\\n'\n",
    "                        content=content+temp_c[i]\n",
    "                    else:\n",
    "                        content=content+temp_c[i]\n",
    "                if content == '':\n",
    "                    content=' '\n",
    "                else:\n",
    "                    content=re.sub(string=content,pattern='\\xa0',repl=' ')\n",
    "                    content=emoji.demojize(content)\n",
    "        except:\n",
    "            content=' '\n",
    "        try:\n",
    "            img=''\n",
    "            temp_img=[s.find('img').get('src') for s in soup.find_all('figure')]\n",
    "            for i,s in enumerate(temp_img,start=1):\n",
    "                if i != len(temp_img):\n",
    "                    img=img+s+';'\n",
    "                else:\n",
    "                    img=img+s\n",
    "            if img == '':\n",
    "                img=' '\n",
    "        except:\n",
    "            img=' '\n",
    "        return pd.DataFrame({'title':title,'author':author,'category':' ','url':article_url,\n",
    "                             'content':content,'dt_publish':dt_publish,'img':img,\n",
    "                            'Source':'Simple_Flying'},\n",
    "                           columns=['author','dt_publish','title','content','url','img','Source','category'],index=[index])\n",
    "    elif mode ==2: #Reuters\n",
    "        a=requests.get(article_url,\n",
    "                       headers={'User-Agent':np.random.choice(user_agent)},\n",
    "                       timeout=20)\n",
    "        if a.status_code !=200:\n",
    "            s=datetime.strftime(datetime.now(),'%Y-%m-%d %H:%M:%S')\n",
    "            logging.error(s+':'+' status_code is not 200,url of article is : '+article_url)\n",
    "        soup=BeautifulSoup(a.text,'lxml')\n",
    "        temp=soup.find(class_='ArticleHeader_content-container')\n",
    "        #category\n",
    "        try:\n",
    "            category=temp.find('a').text\n",
    "        except:\n",
    "            print('Can not find category of article,please check.')\n",
    "            category=' '\n",
    "        #dt_publish only date.\n",
    "        try:\n",
    "            dt_publish=re.search(string=temp.find(class_='ArticleHeader_date').text,\n",
    "                                 pattern='(.+\\d+) /').group(1)\n",
    "        # date contain hours and minute\n",
    "        # dt_publish=re.search(string=temp.find(class_='ArticleHeader_date').text,\n",
    "        #                      pattern='.+[AP]*M').group(0)\n",
    "            dt_publish=datetime.strftime(datetime.strptime(dt_publish,'%B %d, %Y'),'%Y/%m/%d')\n",
    "        except:\n",
    "            print('Can not find publish time of article,please check.')\n",
    "            dt_publish=' '\n",
    "        #title\n",
    "        try:\n",
    "            title=temp.find('h1').text\n",
    "        except:\n",
    "            print('Can not find title of article,please check')\n",
    "            title=' '\n",
    "        #author\n",
    "        try:\n",
    "            author=soup.find('div',class_='BylineBar_byline').text\n",
    "        except:\n",
    "            print('Can not find author of article,please check')\n",
    "            author=' '\n",
    "        #content\n",
    "        try:\n",
    "            content=''\n",
    "            for s in soup.find(class_='StandardArticleBody_body').find_all('p'):\n",
    "                content=content+s.text+'\\n'\n",
    "            content=re.sub(string=content,pattern='\\xa0',repl='')\n",
    "            content=re.sub(string=content,pattern='\\n$',repl='')\n",
    "        except:\n",
    "            content=' '\n",
    "            print('Can not find content of article,please check')\n",
    "        #lazy image\n",
    "        try:\n",
    "            img=soup.find('div',class_='LazyImage_container').find('img').get('src')\n",
    "            img=re.sub(string=img,pattern='w=20',repl='w=1280')\n",
    "        except:\n",
    "            print('Can not find image of article,please check')\n",
    "            img=' '\n",
    "        return(pd.DataFrame({'title':title,\n",
    "                              'author':author,\n",
    "                              'category':category,\n",
    "                              'url':article_url,\n",
    "                              'content':content,\n",
    "                              'dt_publish':dt_publish,\n",
    "                              'img':img,\n",
    "                              'Source':'Reuters'},\n",
    "                            columns=['author','dt_publish','category','title','content','url','img','Source'],\n",
    "                            index=[index]))\n",
    "    else: #FlightGlobal\n",
    "    #flightgobal防護機制\n",
    "    #需要裝成瀏覽器，否則直接拒絕\n",
    "    #有些需要登入才給你看的文章(可能是他們認為重要的或隨機選文章)\n",
    "        a=requests.get(article_url,timeout=20,\n",
    "                       headers={'User-Agent':np.random.choice(user_agent),\n",
    "                                'Host':'www.flightglobal.com'})\n",
    "        if a.status_code !=200:\n",
    "            s=datetime.strftime(datetime.now(),'%Y-%m-%d %H:%M:%S')\n",
    "            logging.error(s+':'+' status_code is not 200,url of article is : '+article_url)\n",
    "        soup=BeautifulSoup(a.text,'lxml')\n",
    "        temp=soup.find('div',class_='container titleByline')\n",
    "        #category\n",
    "        try:\n",
    "            category=temp.find('div',class_='storyPrimaryNav').text\n",
    "        except:\n",
    "            print('Can not find category of article,please check.')\n",
    "            category=' '\n",
    "        #dt_publish only date.\n",
    "        try:\n",
    "            dt_publish=datetime.strptime(temp.find(class_='date').text,'%Y-%m-%dT%H:%M:%S%z')\n",
    "            dt_publish=datetime.strftime(dt_publish,'%Y/%m/%d')\n",
    "        except:\n",
    "            print('Can not find publish time of article,please check.')\n",
    "            dt_publish=' '\n",
    "        #title\n",
    "        try:\n",
    "            title=temp.find('h1').text\n",
    "        except:\n",
    "            print('Can not find title of article,please check')\n",
    "            title=' '\n",
    "        #author\n",
    "        try:\n",
    "            author=temp.find(class_='author').text\n",
    "            author=re.sub(pattern='^By |^by ',repl='',string=author)\n",
    "        except:\n",
    "            print('Can not find author of article,please check')\n",
    "            author=' '\n",
    "        #content\n",
    "        try:\n",
    "            content=''\n",
    "            for s in soup.find(class_='storytext').find_all('p',recursive=False):\n",
    "                content=content+s.text+'\\n'\n",
    "            content=re.sub(string=content,pattern='\\xa0',repl='')\n",
    "            content=re.sub(string=content,pattern='\\n$',repl='')\n",
    "        except:\n",
    "            content=' '\n",
    "            print('Can not find content of article,please check')\n",
    "        #lazy image\n",
    "        try:\n",
    "            img=soup.find(class_='storytext').find(class_='image_size_full').find('img').get('data-src')\n",
    "        except:\n",
    "            print('Can not find image of article,please check')\n",
    "            img=' '\n",
    "        return(pd.DataFrame({'title':title,\n",
    "                              'author':author,\n",
    "                              'category':category,\n",
    "                              'url':article_url,\n",
    "                              'content':content,\n",
    "                              'dt_publish':dt_publish,\n",
    "                              'img':img,\n",
    "                              'Source':'FlightGlobal'},\n",
    "                            columns=['author','dt_publish','category','title','content','url','img','Source'],\n",
    "                            index=[index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    D=pd.DataFrame(columns=['author','dt_publish','title','category','content','url','img','Source'])\n",
    "    ind=0\n",
    "    for s in Links1:\n",
    "        D=pd.concat([D,crawl_info(article_url=s,index=ind,mode=1)])\n",
    "        ind=ind+1\n",
    "    for s in Links2:\n",
    "        D=pd.concat([D,crawl_info(article_url=s,index=ind,mode=2)])\n",
    "        ind=ind+1\n",
    "    for s in Links3:\n",
    "        D=pd.concat([D,crawl_info(article_url=s,index=ind,mode=3)])\n",
    "        ind=ind+1\n",
    "except:\n",
    "    logging.error('Function:crawl_info is wrong,please check.',exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_new=D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_new=D_new[~D_new.duplicated(subset=['url'])]\n",
    "D_new=D_new.sort_values(by=['dt_publish'],ascending=True)\n",
    "D_new=D_new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每天寫入SQL資料庫\n",
    "import pymysql\n",
    "import MySQLdb\n",
    "from sqlalchemy import create_engine\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pymysql.connect(\n",
    "    host='ip',\n",
    "    port='port',\n",
    "    user='username',\n",
    "    password='password',\n",
    "    db='dbname')\n",
    "engine = create_engine('mysql+mysqldb://username:password@ip:port/dbname?charset=utf8mb4')\n",
    "all_table=pd.read_sql_query('Show tables',conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor=conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert new data in table news\n",
    "table_name='news'\n",
    "try:\n",
    "    #有檔案\n",
    "    #讀取舊檔\n",
    "    D_old=pd.read_sql_table(table_name,con=engine)\n",
    "    #修改從sql讀取出來的datetime格式\n",
    "    D_old['dt_publish']=D_old['dt_publish'].astype('str')\n",
    "    #舉例:將2010-01-01轉變成2010/01/01 \n",
    "    D_old['dt_publish']=[re.sub(string=s,pattern='-',repl='/') for s in D_old['dt_publish']]\n",
    "    #找尋全新資料(以作者、發文時間、標題合起來當作primary key)\n",
    "    pos=pd.merge(D_new,D_old,on=['title','author','dt_publish'],how='left',indicator=True)['_merge'] == 'left_only'\n",
    "    pos=pos.values.tolist()\n",
    "    if sum(pos) == 0:\n",
    "        print('No new data.')\n",
    "    else:\n",
    "        D_new=D_new.iloc[pos,:]\n",
    "        #去除從不同地方抓到同樣的文章\n",
    "        D_new=D_new[~D_new.duplicated(subset=['title','author','dt_publish'])]\n",
    "        D_new=D_new.sort_values(by=['dt_publish'],ascending=True)\n",
    "        D_new=D_new.reset_index(drop=True)\n",
    "        #存入新資料\n",
    "        for r in range(0,D_new.shape[0]):\n",
    "            cursor.execute('insert into news(title,content,url,dt_publish,author,Source,category) values (%s,%s,%s,%s,%s,%s,%s)',\n",
    "                          (D_new.iloc[r,6],D_new.iloc[r,3],D_new.iloc[r,7],\n",
    "                           D_new.iloc[r,4],D_new.iloc[r,1],D_new.iloc[r,0],\n",
    "                           D_new.iloc[r,2]))\n",
    "        conn.commit()    \n",
    "except:\n",
    "    logging.warning('File to SQL cannot be done due to:',exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordsegment as ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_search(txt,keyword):\n",
    "    #preprocess\n",
    "    keyword_temp=[ws.clean(k) for k in keyword]\n",
    "    pat=''\n",
    "    for i,k in enumerate(keyword_temp):\n",
    "        if len(keyword_temp)>1:\n",
    "            if i not in[len(keyword_temp)-1]:\n",
    "                pat=pat+k+'|'\n",
    "            else:\n",
    "                pat=pat+k\n",
    "        else:\n",
    "            pat=k\n",
    "    txt=ws.clean(txt)\n",
    "    #find if keyword in article or not\n",
    "    find_key=list(set(re.findall(string=txt,pattern=pat)))\n",
    "    #which keyword in the ariticle and store boolean logic\n",
    "    result=[s in find_key for s in keyword_temp]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data contains nid\n",
    "def restructure_content_key(dat):\n",
    "    #從SQL讀取keyword表格\n",
    "    try:\n",
    "        key_w=pd.read_sql_table('keyword',con=engine)\n",
    "        keyword=key_w['keyword'].values.tolist()\n",
    "    except:\n",
    "        logging.error('Can not read keyword table from sql ,please check')\n",
    "    Final=pd.DataFrame(columns=['nid','kid'])\n",
    "    for i in range(0,dat.shape[0]):\n",
    "        nid=dat['nid'].iloc[i]\n",
    "        txt=dat['content'].iloc[i]\n",
    "        temp=keyword_search(txt=txt,keyword=keyword)\n",
    "        if sum(temp) !=0:\n",
    "            Final=pd.concat([Final,\n",
    "                             pd.DataFrame({'nid':nid,'kid':key_w['kid'].iloc[temp].values})])\n",
    "        else:\n",
    "            continue\n",
    "    Final=Final.reset_index(drop=True)\n",
    "    return(Final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find relation of article and keyword.And insert into table news_keyword\n",
    "table_name='news_dealt'\n",
    "try:\n",
    "    #有檔案\n",
    "    #讀取舊檔\n",
    "    Relate_old=pd.read_sql_table(table_name,con=engine)\n",
    "    #找尋新資料\n",
    "    D_new=pd.read_sql_query('select * from news',engine)\n",
    "    temp=list(set(D_new['nid'].values.tolist())-set(Relate_old['nid'].values.astype('int').tolist()))\n",
    "    if len(temp) == 0:\n",
    "        print('Do not need to take any action because no new article')\n",
    "    else:\n",
    "        pos=[i for i,s in enumerate(D_new.nid) if s in temp]\n",
    "        D_new=D_new.iloc[pos,:]\n",
    "        D_new=D_new.reset_index(drop=True)\n",
    "        #紀錄資料以處理\n",
    "        for r in range(0,D_new.shape[0]):\n",
    "            cursor.execute('insert into news_dealt(nid) values (%s)',\n",
    "                          (str(D_new['nid'].iloc[r])))\n",
    "        conn.commit()\n",
    "        #存入新資料\n",
    "        D_new=restructure_content_key(D_new)\n",
    "        if D_new.shape[0] == 0:\n",
    "            print('Do not need to take any action because all keywords not in all articles')\n",
    "        else:\n",
    "            for r in range(0,D_new.shape[0]):\n",
    "                cursor.execute('insert into news_keyword(nid,kid) values (%s,%s)',\n",
    "                              (D_new.iloc[r,0],D_new.iloc[r,1]))\n",
    "            conn.commit()\n",
    "    conn.close()\n",
    "except:\n",
    "    logging.warning('News_keyword to SQL cannot be done due to:',exc_info=True)\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_table('news_keyword',engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_table('news',engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
